= Classifier Documentation =

[[PageOutline]]

This document describes the current architecture of the Peerworks classification system.  It doesn't discuss the details of the classification algorithm itself, instead it focuses on how that algorithm is used to process items, integrate with other applications and scale to meet the demands of classifying items using many users' tags.

== Overview ==

The classifier is built in the C programming language and is targeted at the POSIX platform.  Typically, the classifier will run as a daemon process. The daemon process includes an embedded HTTP server providing a web interface for controlling classification jobs. 

A typical job processing chain consists of a front end sending a REST request to the classifier to perform the classification for a given tag.  This job is added to the classifiers internal queue and a reference to the job's progress status URL is returned to the front-end.  While the job is being processed the front-end can perform GET requests on the returned URL to get updates to the status of the job.  

When the classifier starts processing the job, it first fetches the training document for the tag.  This document contains the items used the user has provided as manual examples for the tag.  The classifier then builds a tagger that is used internally to perform classification.  The tagger is run over each item in the classifier's internal cache to produce a probability that the item is within that tag.  For each item where the probability is over a certain threshold, the item is added to a classifier taggings document which is then sent back to the front-end for storage and updating of the display.

There are 4 main components to the classifier:

 * HTTP server
 * Item Cache
 * Tagger Cache
 * Classification Engine

The following diagram shows their relationships:

[[Image(http://trac.winnow.peerworks.org/raw-attachment/wiki/ClassifierSoftwareDocumentation/MajorComponents.png)]]

Next this document provides details on each of these components, then details of the protocols used, and finally, a discussion of some of the scaling issues we will face.

=== Definitions ===

The following terms are used within this document:

 tagger::
 item::
 classifier::
 example::
 tag document::

=== Installation Instructions ===

Installation instructions are provided as part of the distribution. See the [https://github.com/seangeo/classifier/tree/master/INSTALL INSTALL] file.

== HTTP Server ==

The Classifier includes an embedded HTTP server that is used to control classification jobs and add new items to the classifier's Item Cache.

=== Classification Job Control ===

The HTTP interface for classification job control allows a user to start, query and stop a classification job. This is all done through a REST style interface where jobs are represented as resources within the classifier.  All resources are represented in XML. All operations may return HTTP status codes in the case of something going wrong during the processing of the request.

==== Starting a job ====

To start a job, a caller needs to seed a job description to the classifier using a HTTP post to /classifier/jobs.  A reference to the resource URL for the job is then returned when the classifier successfully creates the job.  

For example, the follow POST request with create a job for the tag defined at http://example.org/tag.atom:

{{{
POST /classifier/jobs HTTP/1.1
Host: localhost

<?xml version="1.0" ?>
<job>
	<tag-url>http://example.org/tag.atom</tag-url>
</job>


}}}

If the job is successfully created the classifier will return a 201 Created response like this:

{{{
HTTP/1.1 201 Created
Date: Fri, 7 Oct 2005 17:17:11 GMT
Content-Length: nnn
Content-Type: application/xml
Location: http://localhost/classifier/jobs/JOBID
}}}

==== Job Status Reporting ====

The URL provides in the location header returned by classifier is the resource URL for the job.  This URL can be used to query the classifier on the status of the job using the HTTP GET method.  

The classification job description shows the job's progress using a state attribute.  The state attribute can be one of the follow values:

 Waiting:: The job is in the queue waiting for a worker thread to start it.
 Training:: The tagger for the job is being trained with the examples provided in the tag definition. Training involves combining the features in each of the positive and negative examples with the random background to end up with an index of feature identifiers and probabilities for the tagger.  The details of how this computed and combined is left up to the classifier implementation.
 Classifying:: The job is classifying each item in the item cache.
 Inserting:: The job is sending the resulting items and probabilities to the callback URL provided by the tagger.
 Complete:: The job is complete!
 Error:: An error occurred while processing the job.

The description also includes these attributes:

 progress:: Percentage of job completed.
 duration:: Number of seconds the job has been running for.
 error-msg (Optional):: A human readable error message when the job state is Error. 

For example, the follow GET request:

{{{
GET /classifier/jobs/JOBID
Host: localhost
}}}

Will return an XML document describing the status of the job identified by JOBID. 

For example:

{{{
HTTP/1.1 200 Ok
Date: Fri, 7 Oct 2005 17:17:11 GMT
Content-Length: nnn
Content-Type: application/xml

<?xml version="1.0" ?>
<job>
	<id>JOBID</id>
	<progress>10.0</progress>
	<status>Training</status>
	<duration>0.1</duration>
	<error-msg></error-msg>
</job>

}}}

==== Cancelling and Deleting Jobs ====

When a job is complete or a user wants to cancel it, the client should send a delete request to the job's URL. For example:

{{{
DELETE /classifier/jobs/JOBID HTTP/1.1
Host: localhost
}}}

This will cancel the job if it is still running or if it is complete it will delete the job.  After a DELETE has been send to a job URL, subsequent GETs on that URL will return a HTTP 404 response.

=== Parameters ===

The follow command line parameters are relevant to the HTTP server.

{{{
-p, --port N     the port to run the HTTP server on
    -a, --allowed_ip IP_ADDRESS
                     An IP address to allow to make HTTP requests
                     Default: any

}}}

''Q: Is there a mechanism that expires and purges old jobs that were never issued a delete request? -Don''

''A: No. Jobs the classifier generates, such those that classify new items for every tag, are cleaned up automatically, but jobs created by clients hang around until they get a delete. This should be changed to give them a timeout, e.g. if they were finished an hour ago but are still hanging around they get deleted automatically. I just created #823 for this. -Sean''

== Classification Engine ==

The Classification Engine is the core component within the classifier. It manages an internal queue of classification jobs and a number of worker threads that process those jobs. It also maintains an index of current jobs referenced by their job id.

Typically a job is added to the classification engine by the HTTP server front end.  When this happens the job is assigned a UUID as its id and added to the back of the job queue.  When a worker thread is available it takes the first job on the front of the queue. Using the tag URL that is specified in the job, the worker fetches the tagger from the Tagger Cache and uses it to classify each item in the Item Cache.  Once all the items have been classified, the worker filters out the items that were classified below the probability threshold (default of 0.9) and then sends the resulting list of items and their classification probabilities to the call-back URL provided by the tagger.  The worker thread is then finished with the job and can move on the next job in the queue.

=== Different Job Types ===

There are two different job types within the classification engine:

All items job::
This type of job will classify all the items in the in-memory item cache. 
It is the default job type.
New items job::
This type of job will only classify items that have been added to the in-memory cache since the last time the given tag was classified. This is the type of job that is created in response to cache updating operations.

=== Error Reporting ===

Errors are reported using error constants and error messages.  The following errors constants are supported:

 NO_SUCH_TAG:: The tag could not be found or the tag document could not be retrieved.
 MISSING_ITEM_TIMEOUT:: Some items used as examples in the tag were missing and the job timed out while waiting to add them to the item database.

=== Parameters ===

The following command line parameters are relevant to the behaviour of the classification engine.

{{{
-n, --worker-threads N
                    number of threads for processing jobs
                    Default: 1

-t, --positive-threshold N
                    probability threshold for considering a tag to be
                    applied to and item
                    Default: 0

    --performance-log FILE
                    location of the file in which to write job timings

    --tag-index URL
                    URL which provides an index of the tags to classify

    --missing-item-timeout N
                    Number of seconds to wait for missing items to be added
                    before a job depending on them is canceled and an error
                    is returned instead                     Default: 60 seconds

}}}

== Tagger Cache ==

The Tagger Cache provides a cache of the taggers that the classifier can use to classify items.  Taggers are stored in the cache using the URL of their training document as the key.

The classification worker threads will access the get_tagger function of the tagger cache.  This function operates as described in the following diagram:

[[Image(http://trac.winnow.peerworks.org/raw-attachment/wiki/ClassifierSoftwareDocumentation/TaggerCacheFlowChart.png)]]

=== Tagger Retrieval ===

When a tagger is retrieved from the tagger cache, the cache will attempt to update the tagger definition by fetching the document from the tagger's URL.  The tagger cache will include an IF-MODIFIED_SINCE header in the request and the tagger will only be updated if the response to the request indicates that the tag definition has changed since it was last retrieved.

=== Building Taggers ===

The tagger cache is also responsible for building taggers from the definition documents.  This involves fetching all the items defined as examples in the document, training the tagger with those examples and pre-calculating all the token probabilities for the tokens in the trained tagger.  The taggers returned by the tagger cache are always in the precomputed state which means they are ready to be used in classification without any further processing.  

In some cases, examples defined in a tagger document may not exist in the classifier's item cache.  When this occurs the tagger cache will add these items to the item cache, place the tagger in the "partially trained" state and return an error.  Future calls to get a tagger in this state will each attempt to train the tagger with the missing items.  When the items have been successfully added to the cache the tagger's training will be completed and the tagger will be checked out and returned to the next caller.  This allows jobs for partially trained taggers to just be added to the back of the job queue and processed again when another worker gets the job off the queue.

=== Locking ===

The Tagger Cache includes a locking mechanism that ensures each tagger can only be used by one thread at a time.  This is implemented as a checkout mechanism.  When a thread gets a tagger from the cache the cache first checks that the tagger has not been checked out by another thread. If the tagger is available it is marked as checked out and returned to the caller. If the tagger is checked out already an error is returned to the caller.  When a thread is finished with the tagger is must call the {{{release_tagger}}} function which checks the tagger back in and makes it available for to other threads.

=== Persistence ===

The Tagger Cache does not currently support any form of persistence.

=== Tag Index ===

The Tagger Cache also provides a facility for fetching an index of tag urls.  The index is expected to be an atom document containing a list of tags as entries with each entry containing a link to the tag's training document.  The cache will apply the IF-MODIFIED-SINCE HTTP caching functional to this index too.  The index allows the system to get a list of all the tags that may be classified in another system in order to generate automatic classification jobs.  For example, this facility is used when new items are published to the classifier's item cache, the classifier will generate new item classification jobs for each tag in Winnow.

== Item Cache ==

The Item Cache provides a cache of the items that the classifier can use in training and classification and their extracted features that are used in classification.  The Item Cache provides two levels of caching. There is a persistent cache of items and their features over a given time period and an in memory cache of a subset of the persistent cache that contains the items that will actually be classified.

The length of the subset to use for the in-memory cache can be specified as a command-line parameter, the default is to keep items from the last 30 days in memory.  When the classifier is first started it will load these items into memory from the persistent cache. The in-memory cache maintains two indexes for these items, one that allows fast retrieval of an item given it's id and one which orders items by time.  The first index is used for fast retrieval of training items if they happen to be in the in-memory cache and the second index is used so that classification occurs in chronological order and when a job only needs to process items added since a given time it can stop processing when it reaches that point.

The in-memory item cache only contains the id, the extracted features and a timestamp for the item.  Extracted features are stored in a Judy array that maps the feature id to its value. In our current case the feature id is the atomized version of a token, i.e. an int32, and the value represents the frequency of the token in that item. On the other hand, the persistent cache stores both the extracted features and the raw data of the item.

=== Updating the cache ===

The Item Cache is populated via the Atom Publishing protocol.  Each item with the cache corresponds to an Atom Entry which is published into the cache.  The cache also includes a representation of feeds with are solely used for grouping collections of items together.

As items are published to the cache they use a processing chain within the classifier as shown in this diagram:

[[Image(http://trac.winnow.peerworks.org/raw-attachment/wiki/ClassifierSoftwareDocumentation/ItemCacheUpdateChart.png)]]

In the above diagram, the parallelograms represent internal queues within the classifier. These are used so that tokenization and database insertion can be serialized and performed asynchronously from the Atom Publishing Protocol (APP) requests that add the items. When a APP request adds an item it is immediately saved in its raw state in the SQLite database. This allows the HTTP request-response cycle to be completed very quickly and the tokenization and updating of the in-memory items cache (which requires locking out all classification jobs) to occur in the background.

The Item Cache also provides a callback that is triggered when either a certain number of items have been added to the cache or a certain amount of time has passed since the last item was added to the cache.  By default this is triggered after either 200 items have been added or 60 seconds have passed since the last addition.

=== Purging the cache ===

A separate thread runs nightly which purges the in-memory cache of items older than 30 days. Currently there is no mechanism for purging the persistent cache, but considering this cache is getting quite large, it currently has almost 350K items in it and the database is 3GB is size we probably need to solve this soon.

=== Cache Persistence ===

Cache persistence is implemented using a set of an [SQLite http://www.sqlite.org] databases.  The persistent cache consists of a directory with the following sqlite databases:

{{{
top-level/
  catalog.db
  atom.db
  tokens.db
}}}

The {{{catalog.db}}} database contains the metadata for each item in the cache and the map of token id to token strings.  The atom.db file contains to source atom entry xml for each item in the database. The tokens.db file contains the tokenized form of each item in the database.

The database schema is shown here:

[[Image(/raw-attachment/wiki/ClassifierSoftwareDocumentation/CatalogSchema5.png)]]

All datetime columns are represented as real numbers in the [http://en.wikipedia.org/wiki/Julian_day Julian Day] format as [http://www.sqlite.org/cvstrac/wiki?p=DateAndTimeFunctions recommended] in the SQLite documentation.

Table and column definitions are as follows:

===== entries =====

Contains metadata for each entry in the database.
 
  id:: An integer surrogate key used to identify the item internally to the item cache. 
  full_id:: A URI for the item that is defined in the atom:id element for the item. This is a global identifier for the item.
  updated:: The datetime the item was updated, according to the atom:updated element for the item.
  created_at:: The datetime the item was added to the cache.
	last_used_at:: The datetime the item was last used to train a classifier, or null if never used.

===== random_backgrounds =====

Specifies a list of items in the entries table that should be used for the random background within the classifier.

  entry_id:: Id of the entry to use in the random background.

===== tokens =====

Provides a mapping between the atomized version of a token and the string version of the token.

  id:: The atomized integer version of the token.
  token:: The original textual version of the token.

This table also has unique constraints on each column to ensure that you can't have duplicate tokens or mappings.

===== entry_atom =====

Stores the raw atom xml content for each entry in the catalog.

  id:: The id of the entry. This is a foreign key to the entries.id column.
  atom:: The Atom xml for the entry. This is the raw content of the item that is used to generate tokens.

===== entry_tokens =====

Stores the tokenized representation of the entries in the catalog.

  id:: The id of the entry. This is a foreign key to the entries.id column.
  tokens:: The tokenized representation of the item.

The tokens column is a binary format where each token is represented by six bytes, so the size of the BLOB will be multiples of six.  The first 4 bytes of each token represent a 32 bit integer containing the atomized token id for that feature.  The second two bytes represent a 16 bit short integer that corresponds to the frequency (or value) of that feature within the item.  The token file uses network byte order, big endian, for storage, so programs wishing to read or write token files must perform their own conversions to their native byte order.  The classifier uses the [http://www.gnu.org/software/libc/manual/html_node/Byte-Order.html ntoh*] family of functions to perform this conversion in a portable way.

== Classifier Document Formats ==

There are two document formats that are important to the classifier.  Firstly the Tag Definition Document (TDD) defines the manual examples for a tag, this makes up the positive and negative training used by the classifier.  When a job is created in the classifier, the client provides a URL to the TDD and the classifier will fetch the TDD in order to build a Tagger for classifying items. Secondly the Classifier Tagging Document (CTD) defines the items which the classifier has applied a given tag to and strength for which the tag is applied. The CTD is created by the classifier as the output of the classification process, it is sent to the classifier tagging URL defined in the TDD which was used to create the Tagger that generated the CTD.

Both of these formats are based on the [http://atomenabled.org/developers/syndication/atom-format-spec.php Atom Syndication Format] with some minor extension elements

=== Tag Definition Document ===

The TDD is an Atom Feed document where the items in the feed represent the training items for a tag.  All the semantics of the Atom Syndication Format apply along with some extra conditions and elements. The extensions and conditions will be illustrated by example:

Extension elements specific to the classifier are defined within the classifier's namespace which is {{{http://peerworks.org/classifier}}}. This can be defined on the root element of the feed like so:

<?xml version="1.0" ?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:classifier="http://peerworks.org/classifier">

Next some additional metadata is required at the feed level.

This is an example of the feed level elements for a TDD:

{{{
  <id>http://trunk.mindloom.org:80/seangeo/tags/a-religion</id>
  <title>seangeo:a-religion</title>
  <updated>2008-06-24T18:37:44+09:30</updated>
  <link href="http://trunk.mindloom.org:80/seangeo/tags/a-religion.atom" rel="alternate"/>
  <link href="http://trunk.mindloom.org:80/seangeo/tags/a-religion/training.atom" rel="self"/>

  <!-- The following four elements are required by the classifier -->
  <link href="http://trunk.mindloom.org:80/seangeo/tags/a-religion/classifier_taggings.atom"
             rel="http://peerworks.org/classifier/edit"/>
  <category scheme="http://trunk.mindloom.org:80/seangeo/tags/" term="a-religion"/>
  <classifier:bias>1.2</classifier:bias>
  <classifier:classified>2008-06-25T00:47:36+09:30</classifier:classified>

}}}

The required elements serve the following purposes:
  * The required link element specifies the URL which the Classifier Tagging Document should be sent to when classification is complete.  The link is identified using the {{{http://peerworks.org/classifier/edit}}} value for the rel attribute.  
  * The required category element specifies the scheme and term used for all category elements relating to the tag in both TDD and CTD documents, this means that positive items in the TDD will use the scheme and term and classifier generated taggings in the CTD will use the scheme and term. 
  * The classifier:bias element defines the classification bias as a float, values less than 1.0 will make the classifier more conservative whereas values greater than 1.0 will make it less conservative.
  * The classifier:classified provides the time when the tag was last classified.

While only four of the elements are required by the classifier they should all be provided anyway to ensure compliance with the Atom spec.

Following the feed level metadata are the items that make up the training data for the tag.  These are specified as atom:entry elements using the full content of the items.  The classifier specifies two conditions to identify whether an item is a positive or negative example.

TODO

=== Classifier Tagging Document ===

TODO
